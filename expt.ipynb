{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "246b04ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9faf4da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "681d98ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autopad(k, p=None, d=1):  # kernel, padding, dilation\n",
    "    \"\"\"Pad to 'same' shape outputs.\"\"\"\n",
    "    if d > 1:\n",
    "        k = d * (k - 1) + 1 if isinstance(k, int) else [d * (x - 1) + 1 for x in k]  # actual kernel-size\n",
    "    if p is None:\n",
    "        p = k // 2 if isinstance(k, int) else [x // 2 for x in k]  # auto-pad\n",
    "    return p\n",
    "\n",
    "\n",
    "class Conv(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard convolution module with batch normalization and activation.\n",
    "\n",
    "    Attributes:\n",
    "        conv (nn.Conv2d): Convolutional layer.\n",
    "        bn (nn.BatchNorm2d): Batch normalization layer.\n",
    "        act (nn.Module): Activation function layer.\n",
    "        default_act (nn.Module): Default activation function (SiLU).\n",
    "    \"\"\"\n",
    "\n",
    "    default_act = nn.SiLU()  # default activation\n",
    "\n",
    "    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, d=1, act=True):\n",
    "        \"\"\"\n",
    "        Initialize Conv layer with given parameters.\n",
    "\n",
    "        Args:\n",
    "            c1 (int): Number of input channels.\n",
    "            c2 (int): Number of output channels.\n",
    "            k (int): Kernel size.\n",
    "            s (int): Stride.\n",
    "            p (int, optional): Padding.\n",
    "            g (int): Groups.\n",
    "            d (int): Dilation.\n",
    "            act (bool | nn.Module): Activation function.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p, d), groups=g, dilation=d, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(c2)\n",
    "        self.act = self.default_act if act is True else act if isinstance(act, nn.Module) else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Apply convolution, batch normalization and activation to input tensor.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            (torch.Tensor): Output tensor.\n",
    "        \"\"\"\n",
    "        return self.act(self.bn(self.conv(x)))\n",
    "\n",
    "    def forward_fuse(self, x):\n",
    "        \"\"\"\n",
    "        Apply convolution and activation without batch normalization.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            (torch.Tensor): Output tensor.\n",
    "        \"\"\"\n",
    "        return self.act(self.conv(x))\n",
    "    \n",
    "class Bottleneck(nn.Module):\n",
    "    \"\"\"Standard bottleneck.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, c1: int, c2: int, shortcut: bool = True, g: int = 1, k: tuple[int, int] = (3, 3), e: float = 0.5\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize a standard bottleneck module.\n",
    "\n",
    "        Args:\n",
    "            c1 (int): Input channels.\n",
    "            c2 (int): Output channels.\n",
    "            shortcut (bool): Whether to use shortcut connection.\n",
    "            g (int): Groups for convolutions.\n",
    "            k (tuple): Kernel sizes for convolutions.\n",
    "            e (float): Expansion ratio.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        c_ = int(c2 * e)  # hidden channels\n",
    "        self.cv1 = Conv(c1, c_, k[0], 1)\n",
    "        self.cv2 = Conv(c_, c2, k[1], 1, g=g)\n",
    "        self.add = shortcut and c1 == c2\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Apply bottleneck with optional shortcut connection.\"\"\"\n",
    "        return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))\n",
    "    \n",
    "class C2f(nn.Module):\n",
    "    \"\"\"Faster Implementation of CSP Bottleneck with 2 convolutions.\"\"\"\n",
    "\n",
    "    def __init__(self, c1: int, c2: int, n: int = 1, shortcut: bool = False, g: int = 1, e: float = 0.5):\n",
    "        \"\"\"\n",
    "        Initialize a CSP bottleneck with 2 convolutions.\n",
    "\n",
    "        Args:\n",
    "            c1 (int): Input channels.\n",
    "            c2 (int): Output channels.\n",
    "            n (int): Number of Bottleneck blocks.\n",
    "            shortcut (bool): Whether to use shortcut connections.\n",
    "            g (int): Groups for convolutions.\n",
    "            e (float): Expansion ratio.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.c = int(c2 * e)  # hidden channels\n",
    "        self.cv1 = Conv(c1, 2 * self.c, 1, 1)\n",
    "        self.cv2 = Conv((2 + n) * self.c, c2, 1)  # optional act=FReLU(c2)\n",
    "        self.m = nn.ModuleList(Bottleneck(self.c, self.c, shortcut, g, k=((3, 3), (3, 3)), e=1.0) for _ in range(n))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass through C2f layer.\"\"\"\n",
    "        y = list(self.cv1(x).chunk(2, 1))\n",
    "        y.extend(m(y[-1]) for m in self.m)\n",
    "        return self.cv2(torch.cat(y, 1))\n",
    "\n",
    "    def forward_split(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass using split() instead of chunk().\"\"\"\n",
    "        y = self.cv1(x).split((self.c, self.c), 1)\n",
    "        y = [y[0], y[1]]\n",
    "        y.extend(m(y[-1]) for m in self.m)\n",
    "        return self.cv2(torch.cat(y, 1))\n",
    "    \n",
    "class C3k2(C2f):\n",
    "    \"\"\"Faster Implementation of CSP Bottleneck with 2 convolutions.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, c1: int, c2: int, n: int = 1, c3k: bool = False, e: float = 0.5, g: int = 1, shortcut: bool = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize C3k2 module.\n",
    "\n",
    "        Args:\n",
    "            c1 (int): Input channels.\n",
    "            c2 (int): Output channels.\n",
    "            n (int): Number of blocks.\n",
    "            c3k (bool): Whether to use C3k blocks.\n",
    "            e (float): Expansion ratio.\n",
    "            g (int): Groups for convolutions.\n",
    "            shortcut (bool): Whether to use shortcut connections.\n",
    "        \"\"\"\n",
    "        super().__init__(c1, c2, n, shortcut, g, e)\n",
    "        self.m = nn.ModuleList(\n",
    "            C3k(self.c, self.c, 2, shortcut, g) if c3k else Bottleneck(self.c, self.c, shortcut, g) for _ in range(n)\n",
    "        )\n",
    "\n",
    "class C3(nn.Module):\n",
    "    \"\"\"CSP Bottleneck with 3 convolutions.\"\"\"\n",
    "\n",
    "    def __init__(self, c1: int, c2: int, n: int = 1, shortcut: bool = True, g: int = 1, e: float = 0.5):\n",
    "        \"\"\"\n",
    "        Initialize the CSP Bottleneck with 3 convolutions.\n",
    "\n",
    "        Args:\n",
    "            c1 (int): Input channels.\n",
    "            c2 (int): Output channels.\n",
    "            n (int): Number of Bottleneck blocks.\n",
    "            shortcut (bool): Whether to use shortcut connections.\n",
    "            g (int): Groups for convolutions.\n",
    "            e (float): Expansion ratio.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        c_ = int(c2 * e)  # hidden channels\n",
    "        self.cv1 = Conv(c1, c_, 1, 1)\n",
    "        self.cv2 = Conv(c1, c_, 1, 1)\n",
    "        self.cv3 = Conv(2 * c_, c2, 1)  # optional act=FReLU(c2)\n",
    "        self.m = nn.Sequential(*(Bottleneck(c_, c_, shortcut, g, k=((1, 1), (3, 3)), e=1.0) for _ in range(n)))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass through the CSP bottleneck with 3 convolutions.\"\"\"\n",
    "        return self.cv3(torch.cat((self.m(self.cv1(x)), self.cv2(x)), 1))\n",
    "\n",
    "class C3k(C3):\n",
    "    \"\"\"C3k is a CSP bottleneck module with customizable kernel sizes for feature extraction in neural networks.\"\"\"\n",
    "\n",
    "    def __init__(self, c1: int, c2: int, n: int = 1, shortcut: bool = True, g: int = 1, e: float = 0.5, k: int = 3):\n",
    "        \"\"\"\n",
    "        Initialize C3k module.\n",
    "\n",
    "        Args:\n",
    "            c1 (int): Input channels.\n",
    "            c2 (int): Output channels.\n",
    "            n (int): Number of Bottleneck blocks.\n",
    "            shortcut (bool): Whether to use shortcut connections.\n",
    "            g (int): Groups for convolutions.\n",
    "            e (float): Expansion ratio.\n",
    "            k (int): Kernel size.\n",
    "        \"\"\"\n",
    "        super().__init__(c1, c2, n, shortcut, g, e)\n",
    "        c_ = int(c2 * e)  # hidden channels\n",
    "        # self.m = nn.Sequential(*(RepBottleneck(c_, c_, shortcut, g, k=(k, k), e=1.0) for _ in range(n)))\n",
    "        self.m = nn.Sequential(*(Bottleneck(c_, c_, shortcut, g, k=(k, k), e=1.0) for _ in range(n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4093bbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 2, 2, 2, 2]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[*(2 for _ in range(5))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e31bd410",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Backbone(nn.Module):\n",
    "    def __init__(self, d, w, mc):\n",
    "        super(Backbone, self).__init__()\n",
    "        self.cv_0 = Conv(3, int((min(64, mc)) * w), k=3, s=2)\n",
    "        self.cv_1 = Conv(int((min(64, mc)) * w), int(min(128, mc) * w), k=3, s=2)\n",
    "        self.c3k2_2 = C3k2(int(min(128, mc) * w), int(min(256, mc) * w), n=int(2*d), c3k=False, e=0.25)\n",
    "        self.cv_3 = Conv(int(min(256, mc) * w), int(min(256, mc) * w), k=3, s=2)\n",
    "        self.c3k2_4 = C3k2(int(min(256, mc) * w), int(min(512, mc) * w), n=int(2*d), c3k=False, e=0.25)\n",
    "        self.cv_5 = Conv(int(min(512, mc) * w), int(min(512, mc) * w), k=3, s=2)\n",
    "        self.c3k2_6 = C3k2(int(min(512, mc) * w), int(min(512, mc) * w), n=int(2*d), c3k=True)\n",
    "        self.cv_7 = Conv(int(min(512, mc) * w), int(min(1024, mc) * w), k=3, s=2)\n",
    "        self.c3k2_8 = C3k2(int(min(1024, mc) * w), int(min(1024, mc) * w), n=int(2*d), c3k=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cv_0(x)\n",
    "        x = self.cv_1(x)\n",
    "        x = self.c3k2_2(x)\n",
    "        x = self.cv_3(x)\n",
    "        out_4 = self.c3k2_4(x)\n",
    "        x = self.cv_5(out_4)\n",
    "        out_6 = self.c3k2_6(x)\n",
    "        x = self.cv_7(out_6)\n",
    "        out_8 = self.c3k2_8(x)\n",
    "        return out_4, out_6, out_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dec0814d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 3, 640, 640)\n",
    "model = Backbone(0.5, 0.25, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4331ea03",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1, f2, f3 = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "93013df4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 20, 20])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "49fbf0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.modules import SPPF, C2PSA, Conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a096e57f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module file: /Users/siddharthmishra/Desktop/from_scratch/yolov11/src/modules.py\n",
      "exported names: ['Attention', 'Bottleneck', 'C2PSA', 'C2f', 'C3', 'C3k', 'C3k2', 'Conv', 'PSABlock', 'SPPF', 'autopad', 'nn', 'torch']\n",
      "has DWConv: False\n"
     ]
    }
   ],
   "source": [
    "# 1. Where is Python loading src.modules from?\n",
    "import src.modules as m\n",
    "print(\"module file:\", getattr(m, \"__file__\", None))\n",
    "\n",
    "# 2. What names does the module actually expose?\n",
    "print(\"exported names:\", [n for n in dir(m) if not n.startswith(\"_\")])\n",
    "\n",
    "# 3. Is DWConv present according to hasattr?\n",
    "print(\"has DWConv:\", hasattr(m, \"DWConv\"))\n",
    "\n",
    "# 4. If present, show the object\n",
    "if hasattr(m, \"DWConv\"):\n",
    "    print(\"DWConv object:\", m.DWConv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4031baee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neck(nn.Module):\n",
    "    def __init__(self, d, w, mc):\n",
    "        super(Neck, self).__init__()\n",
    "        self.sppf_9 = SPPF(int(min(1024, mc) * w), int(min(1024, mc) * w), k=5)\n",
    "        self.c2psa_10 = C2PSA(int(min(1024, mc) * w), int(min(1024, mc) * w), n=int(2*d))\n",
    "        self.upsample_11 = nn.ConvTranspose2d(int(min(1024, mc) * w), int(min(1024, mc) * w), kernel_size=2, stride=2)\n",
    "        # 12th step is concatenation of 6th step and upsampled 11th step, see it in forward function.\n",
    "        self.c3k2_13 = C3k2(int(min(1024, mc) * w) + int(min(512, mc) * w), int(min(512, mc) * w), n=int(2*d), c3k=False)\n",
    "        self.upsample_14 = nn.ConvTranspose2d(int(min(512, mc) * w), int(min(512, mc) * w), kernel_size=2, stride=2)\n",
    "        # 15th step is concatenation of 4th step and upsampled 14th step, see it in forward function.\n",
    "        self.c3k2_16 = C3k2(int(min(512, mc) * w) + int(min(512, mc) * w), int(min(256, mc) * w), n=int(2*d), c3k=False)\n",
    "        self.conv_17 = Conv(int(min(256, mc) * w), int(min(256, mc) * w), k=3, s=2)\n",
    "        # 18th step is concatenation of 17th step and 13th step, see it in forward function.\n",
    "        self.c3k2_19 = C3k2(int(min(256, mc) * w) + int(min(512, mc) * w), int(min(512, mc) * w), n=int(2*d), c3k=False)\n",
    "        self.conv_20 = Conv(int(min(512, mc) * w), int(min(512, mc) * w), k=3, s=2)\n",
    "        # 21th step is concatenation of 20th step and 10th step, see it in forward function.\n",
    "        self.c3k2_22 = C3k2(int(min(512, mc) * w) + int(min(1024, mc) * w), int(min(1024, mc) * w), n=int(2*d), c3k=True)\n",
    "\n",
    "    def forward(self, backbone_out_4, backbone_out_6, backbone_out_8):\n",
    "        x = self.sppf_9(backbone_out_8)\n",
    "        x = self.c2psa_10(x)\n",
    "        x = self.upsample_11(x)\n",
    "        x = torch.cat((x, backbone_out_6), dim=1) # 12th step\n",
    "        x = self.c3k2_13(x)\n",
    "        x = self.upsample_14(x)\n",
    "        x = torch.cat((x, backbone_out_4), dim=1) # 15th step\n",
    "        out_16 = self.c3k2_16(x)\n",
    "        x = self.conv_17(out_16)\n",
    "        x = torch.cat((x, backbone_out_6), dim=1) # 18th step\n",
    "        out_19 = self.c3k2_19(x)\n",
    "        x = self.conv_20(out_19)\n",
    "        x = torch.cat((x, backbone_out_8), dim=1) # 21th step\n",
    "        out_22 = self.c3k2_22(x)\n",
    "        return out_16, out_19, out_22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "80ff2025",
   "metadata": {},
   "outputs": [],
   "source": [
    "neck = Neck(0.5, 0.25, 1024)\n",
    "n_out1, n_out2, n_out3 = neck(f1, f2, f3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "efbbbdd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 64, 80, 80]),\n",
       " torch.Size([1, 128, 40, 40]),\n",
       " torch.Size([1, 256, 20, 20]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_out1.shape, n_out2.shape, n_out3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd50e4c9",
   "metadata": {},
   "source": [
    "## Heads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c5b50c",
   "metadata": {},
   "source": [
    "### Standard Object Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fe8c4e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DWConv(Conv):\n",
    "    \"\"\"Depth-wise convolution module.\"\"\"\n",
    "\n",
    "    def __init__(self, c1, c2, k=1, s=1, d=1, act=True):\n",
    "        \"\"\"\n",
    "        Initialize depth-wise convolution with given parameters.\n",
    "\n",
    "        Args:\n",
    "            c1 (int): Number of input channels.\n",
    "            c2 (int): Number of output channels.\n",
    "            k (int): Kernel size.\n",
    "            s (int): Stride.\n",
    "            d (int): Dilation.\n",
    "            act (bool | nn.Module): Activation function.\n",
    "        \"\"\"\n",
    "        super().__init__(c1, c2, k, s, g=math.gcd(c1, c2), d=d, act=act)\n",
    "from src.loss_functions import DFL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8308ed45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_anchors(feats, strides, grid_cell_offset=0.5):\n",
    "    \"\"\"Generate anchors from features.\"\"\"\n",
    "    anchor_points, stride_tensor = [], []\n",
    "    assert feats is not None\n",
    "    dtype, device = feats[0].dtype, feats[0].device\n",
    "    for i, stride in enumerate(strides):\n",
    "        h, w = feats[i].shape[2:] if isinstance(feats, list) else (int(feats[i][0]), int(feats[i][1]))\n",
    "        sx = torch.arange(end=w, device=device, dtype=dtype) + grid_cell_offset  # shift x\n",
    "        sy = torch.arange(end=h, device=device, dtype=dtype) + grid_cell_offset  # shift y\n",
    "        sy, sx = torch.meshgrid(sy, sx)\n",
    "        anchor_points.append(torch.stack((sx, sy), -1).view(-1, 2))\n",
    "        stride_tensor.append(torch.full((h * w, 1), stride, dtype=dtype, device=device))\n",
    "    return torch.cat(anchor_points), torch.cat(stride_tensor)\n",
    "\n",
    "def dist2bbox(distance, anchor_points, xywh=True, dim=-1):\n",
    "    \"\"\"Transform distance(ltrb) to box(xywh or xyxy).\"\"\"\n",
    "    lt, rb = distance.chunk(2, dim)\n",
    "    x1y1 = anchor_points - lt\n",
    "    x2y2 = anchor_points + rb\n",
    "    if xywh:\n",
    "        c_xy = (x1y1 + x2y2) / 2\n",
    "        wh = x2y2 - x1y1\n",
    "        return torch.cat([c_xy, wh], dim)  # xywh bbox\n",
    "    return torch.cat((x1y1, x2y2), dim)  # xyxy bbox\n",
    "\n",
    "def bbox2dist(anchor_points, bbox, reg_max):\n",
    "    \"\"\"Transform bbox(xyxy) to dist(ltrb).\"\"\"\n",
    "    x1y1, x2y2 = bbox.chunk(2, -1)\n",
    "    return torch.cat((anchor_points - x1y1, x2y2 - anchor_points), -1).clamp_(0, reg_max - 0.01)  # dist (lt, rb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "054c2468",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class HeadOD(nn.Module):\n",
    "    \"\"\"\n",
    "    YOLO Detect head for object detection models.\n",
    "\n",
    "    This class implements the detection head used in YOLO models for predicting bounding boxes and class probabilities.\n",
    "    It supports both training and inference modes, with optional end-to-end detection capabilities.\n",
    "\n",
    "    Attributes:\n",
    "        dynamic (bool): Force grid reconstruction.\n",
    "        export (bool): Export mode flag.\n",
    "        format (str): Export format.\n",
    "        max_det (int): Maximum detections per image.\n",
    "        shape (tuple): Input shape.\n",
    "        anchors (torch.Tensor): Anchor points.\n",
    "        strides (torch.Tensor): Feature map strides.\n",
    "        xyxy (bool): Output format, xyxy or xywh.\n",
    "        nc (int): Number of classes.\n",
    "        nl (int): Number of detection layers.\n",
    "        reg_max (int): DFL channels.\n",
    "        no (int): Number of outputs per anchor.\n",
    "        stride (torch.Tensor): Strides computed during build.\n",
    "        cv2 (nn.ModuleList): Convolution layers for box regression.\n",
    "        cv3 (nn.ModuleList): Convolution layers for classification.\n",
    "        dfl (nn.Module): Distribution Focal Loss layer.\n",
    "        one2one_cv2 (nn.ModuleList): One-to-one convolution layers for box regression.\n",
    "        one2one_cv3 (nn.ModuleList): One-to-one convolution layers for classification.\n",
    "\n",
    "    Methods:\n",
    "        forward: Perform forward pass and return predictions.\n",
    "        forward_end2end: Perform forward pass for end-to-end detection.\n",
    "        bias_init: Initialize detection head biases.\n",
    "        decode_bboxes: Decode bounding boxes from predictions.\n",
    "        postprocess: Post-process model predictions.\n",
    "\n",
    "    Examples:\n",
    "        Create a detection head for 80 classes\n",
    "        >>> detect = Detect(nc=80, ch=(256, 512, 1024))\n",
    "        >>> x = [torch.randn(1, 256, 80, 80), torch.randn(1, 512, 40, 40), torch.randn(1, 1024, 20, 20)]\n",
    "        >>> outputs = detect(x)\n",
    "    \"\"\"\n",
    "\n",
    "    dynamic = False  # force grid reconstruction\n",
    "    export = False  # export mode\n",
    "    format = None  # export format\n",
    "    max_det = 300  # max_det\n",
    "    shape = None\n",
    "    anchors = torch.empty(0)  # init\n",
    "    strides = torch.empty(0)  # init\n",
    "    xyxy = False  # xyxy or xywh output\n",
    "\n",
    "    def __init__(self, nc: int = 80, ch: tuple = ()):\n",
    "        \"\"\"\n",
    "        Initialize the YOLO detection layer with specified number of classes and channels.\n",
    "\n",
    "        Args:\n",
    "            nc (int): Number of classes.\n",
    "            ch (tuple): Tuple of channel sizes from backbone feature maps.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.nc = nc  # number of classes\n",
    "        self.nl = len(ch)  # number of detection layers\n",
    "        self.reg_max = 16  # DFL channels (ch[0] // 16 to scale 4/8/12/16/20 for n/s/m/l/x)\n",
    "        self.no = nc + self.reg_max * 4  # number of outputs per anchor\n",
    "        self.stride = torch.zeros(self.nl)  # strides computed during build\n",
    "        c2, c3 = max((16, ch[0] // 4, self.reg_max * 4)), max(ch[0], min(self.nc, 100))  # channels\n",
    "        self.cv2 = nn.ModuleList(\n",
    "            nn.Sequential(Conv(x, c2, 3), Conv(c2, c2, 3), nn.Conv2d(c2, 4 * self.reg_max, 1)) for x in ch\n",
    "        )\n",
    "        self.cv3 = nn.ModuleList(\n",
    "                nn.Sequential(\n",
    "                    nn.Sequential(DWConv(x, x, 3), Conv(x, c3, 1)),\n",
    "                    nn.Sequential(DWConv(c3, c3, 3), Conv(c3, c3, 1)),\n",
    "                    nn.Conv2d(c3, self.nc, 1),\n",
    "                )\n",
    "                for x in ch\n",
    "            )\n",
    "        self.dfl = DFL(self.reg_max) if self.reg_max > 1 else nn.Identity()\n",
    "\n",
    "    def forward(self, x: list[torch.Tensor]) -> list[torch.Tensor] | tuple:\n",
    "        \"\"\"Concatenate and return predicted bounding boxes and class probabilities.\"\"\"\n",
    "\n",
    "        for i in range(self.nl):\n",
    "            x[i] = torch.cat((self.cv2[i](x[i]), self.cv3[i](x[i])), 1)\n",
    "        if self.training:  # Training path\n",
    "            return x\n",
    "        y = self._inference(x)\n",
    "        return y if self.export else (y, x)\n",
    "\n",
    "    def _inference(self, x: list[torch.Tensor]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Decode predicted bounding boxes and class probabilities based on multiple-level feature maps.\n",
    "\n",
    "        Args:\n",
    "            x (list[torch.Tensor]): List of feature maps from different detection layers.\n",
    "\n",
    "        Returns:\n",
    "            (torch.Tensor): Concatenated tensor of decoded bounding boxes and class probabilities.\n",
    "        \"\"\"\n",
    "        # Inference path\n",
    "        shape = x[0].shape  # BCHW\n",
    "        x_cat = torch.cat([xi.view(shape[0], self.no, -1) for xi in x], 2)\n",
    "        if self.dynamic or self.shape != shape:\n",
    "            self.anchors, self.strides = (x.transpose(0, 1) for x in make_anchors(x, self.stride, 0.5))\n",
    "            self.shape = shape\n",
    "\n",
    "        if self.export and self.format in {\"saved_model\", \"pb\", \"tflite\", \"edgetpu\", \"tfjs\"}:  # avoid TF FlexSplitV ops\n",
    "            box = x_cat[:, : self.reg_max * 4]\n",
    "            cls = x_cat[:, self.reg_max * 4 :]\n",
    "        else:\n",
    "            box, cls = x_cat.split((self.reg_max * 4, self.nc), 1)\n",
    "\n",
    "        if self.export and self.format in {\"tflite\", \"edgetpu\"}:\n",
    "            # Precompute normalization factor to increase numerical stability\n",
    "            # See https://github.com/ultralytics/ultralytics/issues/7371\n",
    "            grid_h = shape[2]\n",
    "            grid_w = shape[3]\n",
    "            grid_size = torch.tensor([grid_w, grid_h, grid_w, grid_h], device=box.device).reshape(1, 4, 1)\n",
    "            norm = self.strides / (self.stride[0] * grid_size)\n",
    "            dbox = self.decode_bboxes(self.dfl(box) * norm, self.anchors.unsqueeze(0) * norm[:, :2])\n",
    "        else:\n",
    "            dbox = self.decode_bboxes(self.dfl(box), self.anchors.unsqueeze(0)) * self.strides\n",
    "        return torch.cat((dbox, cls.sigmoid()), 1)\n",
    "\n",
    "    def bias_init(self):\n",
    "        \"\"\"Initialize Detect() biases, WARNING: requires stride availability.\"\"\"\n",
    "        m = self  # self.model[-1]  # Detect() module\n",
    "        # cf = torch.bincount(torch.tensor(np.concatenate(dataset.labels, 0)[:, 0]).long(), minlength=nc) + 1\n",
    "        # ncf = math.log(0.6 / (m.nc - 0.999999)) if cf is None else torch.log(cf / cf.sum())  # nominal class frequency\n",
    "        for a, b, s in zip(m.cv2, m.cv3, m.stride):  # from\n",
    "            a[-1].bias.data[:] = 1.0  # box\n",
    "            b[-1].bias.data[: m.nc] = math.log(5 / m.nc / (640 / s) ** 2)  # cls (.01 objects, 80 classes, 640 img)\n",
    "        if self.end2end:\n",
    "            for a, b, s in zip(m.one2one_cv2, m.one2one_cv3, m.stride):  # from\n",
    "                a[-1].bias.data[:] = 1.0  # box\n",
    "                b[-1].bias.data[: m.nc] = math.log(5 / m.nc / (640 / s) ** 2)  # cls (.01 objects, 80 classes, 640 img)\n",
    "\n",
    "    def decode_bboxes(self, bboxes: torch.Tensor, anchors: torch.Tensor, xywh: bool = True) -> torch.Tensor:\n",
    "        \"\"\"Decode bounding boxes from predictions.\"\"\"\n",
    "        return dist2bbox(\n",
    "            bboxes,\n",
    "            anchors,\n",
    "            xywh=xywh and not self.end2end and not self.xyxy,\n",
    "            dim=1,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def postprocess(preds: torch.Tensor, max_det: int, nc: int = 80) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Post-process YOLO model predictions.\n",
    "\n",
    "        Args:\n",
    "            preds (torch.Tensor): Raw predictions with shape (batch_size, num_anchors, 4 + nc) with last dimension\n",
    "                format [x, y, w, h, class_probs].\n",
    "            max_det (int): Maximum detections per image.\n",
    "            nc (int, optional): Number of classes.\n",
    "\n",
    "        Returns:\n",
    "            (torch.Tensor): Processed predictions with shape (batch_size, min(max_det, num_anchors), 6) and last\n",
    "                dimension format [x, y, w, h, max_class_prob, class_index].\n",
    "        \"\"\"\n",
    "        batch_size, anchors, _ = preds.shape  # i.e. shape(16,8400,84)\n",
    "        boxes, scores = preds.split([4, nc], dim=-1)\n",
    "        index = scores.amax(dim=-1).topk(min(max_det, anchors))[1].unsqueeze(-1)\n",
    "        boxes = boxes.gather(dim=1, index=index.repeat(1, 1, 4))\n",
    "        scores = scores.gather(dim=1, index=index.repeat(1, 1, nc))\n",
    "        scores, index = scores.flatten(1).topk(min(max_det, anchors))\n",
    "        i = torch.arange(batch_size)[..., None]  # batch indices\n",
    "        return torch.cat([boxes[i, index // nc], scores[..., None], (index % nc)[..., None].float()], dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0954e264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 40, 40])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_out2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c8ec395b",
   "metadata": {},
   "outputs": [],
   "source": [
    "head = HeadOD(nc=80, ch=(64, 128, 256))\n",
    "out = head([n_out1, n_out2, n_out3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d13c0319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 144, 80, 80]),\n",
       " torch.Size([1, 144, 40, 40]),\n",
       " torch.Size([1, 144, 20, 20]))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].shape, out[1].shape, out[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d08850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "453fe79d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.empty(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c413ffb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloObjectDetectionModel(nn.Module):\n",
    "    def __init__(self, d=0.5, w=0.25, mc=1024, nc=80):\n",
    "        super(YoloObjectDetectionModel, self).__init__()\n",
    "        self.backbone = Backbone(d, w, mc)\n",
    "        self.neck = Neck(d, w, mc)\n",
    "        self.head = HeadOD(nc, ch=(int(min(256, mc) * w), int(min(512, mc) * w), int(min(1024, mc) * w)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        f1, f2, f3 = self.backbone(x)\n",
    "        n_out1, n_out2, n_out3 = self.neck(f1, f2, f3)\n",
    "        out = self.head([n_out1, n_out2, n_out3])\n",
    "        return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
